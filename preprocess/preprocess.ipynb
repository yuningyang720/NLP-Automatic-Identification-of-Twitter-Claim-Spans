{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUER9LmZEGs5IrLNLdB6VY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"evZ30R8FjjCn","executionInfo":{"status":"ok","timestamp":1686598551404,"user_tz":420,"elapsed":20232,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}},"outputId":"2b77776c-0921-4036-bce0-aa4985b7e6b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","from google.colab import drive\n","import os\n","from ast import literal_eval\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/schoolwork/grad/23_spring/CS-263-NLP/final-project/NLP-final-project/implementation/preprocess')"]},{"cell_type":"markdown","source":["# Readable strings\n","convert tokenized sequences into readable text."],"metadata":{"id":"moJpajrXj3ns"}},{"cell_type":"code","source":["# takes a list of tokens and converts it to a string\n","def tokens_to_text(token_list):\n","  output=\"\"\n","  for token in token_list:\n","    output += token\n","  return output\n","\n","# extract the claims from the tokens and the indices\n","def tokens_to_claim_text(token_list, span_start_index, span_end_index):\n","  output = []\n","\n","  for claim_span in zip(span_start_index, span_end_index):\n","    output.append(tokens_to_text(token_list[claim_span[0] : claim_span[1]+1]))\n","\n","  return output\n","\n","# take a dataframe and convert the column with tokens to a column with tweet strings --> returns the new dataframe\n","def new_df_with_strings(df):\n","  tweets = []\n","  for token_list in df[\"tokens\"].values:\n","    tweet = tokens_to_text(token_list)\n","    tweets.append(tweet)\n","\n","  df[\"tweets\"] = tweets\n","  df = df.drop('tokens', axis=1)\n","  df = df[[\"tweets\", \"claim_label\", \"span_start_index\", \"span_end_index\"]]\n","\n","  return df"],"metadata":{"id":"UKK1GTLzmmq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"../data/train.csv\", converters={'tokens':literal_eval, 'span_start_index':literal_eval, 'span_end_index':literal_eval})"],"metadata":{"id":"a5C1ZDO5j0_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i=1\n","token_list = df.loc[i, \"tokens\"]\n","print(df.loc[i, \"tokens\"])\n","print(tokens_to_text(token_list))\n","print(tokens_to_claim_text(df.loc[i, \"tokens\"], df.loc[i, \"span_start_index\"], df.loc[i, \"span_end_index\"]))\n","print(df.loc[i, \"span_start_index\"], df.loc[i, \"span_end_index\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uK4IKvbcsLow","executionInfo":{"status":"ok","timestamp":1686599318151,"user_tz":420,"elapsed":160,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}},"outputId":"f2151a97-e468-485c-99c3-a2f7d2a2b03c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['RT', ' @Coach_Brod', ': If', ' you', ' have', ' ever', ' sucked', ' titties', ' in', ' swan', ' lake', ' you', ' are', ' immune', ' to', ' the', ' corona', ' virus']\n","RT @Coach_Brod: If you have ever sucked titties in swan lake you are immune to the corona virus\n","[': If you have ever sucked titties in swan lake you are immune to the corona virus']\n","[2] [17]\n"]}]},{"cell_type":"markdown","source":["# create new datasets\n","convert token lists in original dataset to strings"],"metadata":{"id":"2OON6h-_8wo0"}},{"cell_type":"code","source":["if not os.path.exists(\"../data/train-converted.csv\") or not os.path.exists(\"../data/dev-converted.csv\"):\n","  # read original files\n","  df_train = pd.read_csv(\"../data/train.csv\", converters={'tokens':literal_eval, 'span_start_index':literal_eval, 'span_end_index':literal_eval})\n","  df_dev = pd.read_csv(\"../data/dev.csv\", converters={'tokens':literal_eval, 'span_start_index':literal_eval, 'span_end_index':literal_eval})\n","\n","  # convert train.csv to strings\n","  df_train = new_df_with_strings(df_train)\n","  df_train.to_csv(\"../data/train-converted.csv\", index=False)\n","\n","  # convert dev.csv to strings\n","  df_dev = new_df_with_strings(df_dev)\n","  print(df_dev.shape)\n","  df_dev.to_csv(\"../data/dev-converted.csv\", index=False)"],"metadata":{"id":"74UbsdAUq9u6"},"execution_count":null,"outputs":[]}]}