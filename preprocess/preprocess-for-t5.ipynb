{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPIAvpfZYtJWQtEj9DDx3D2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"RjXxgJ7gQgnm"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQr-Z7enPsGv","executionInfo":{"status":"ok","timestamp":1686701664450,"user_tz":420,"elapsed":21147,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}},"outputId":"96b95f58-922e-42b3-de87-06330c3bb8d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","from google.colab import drive\n","import os\n","from ast import literal_eval\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/schoolwork/grad/23_spring/CS-263-NLP/final-project/NLP-final-project/implementation/preprocess')"]},{"cell_type":"markdown","source":["# Functions"],"metadata":{"id":"SnEGez7mQlYi"}},{"cell_type":"code","source":["# convert a token list to a string\n","def tokens_to_text(token_list):\n","  output=\"\"\n","  for token in token_list:\n","    output += token\n","  return output\n","\n","# convert token list to string without uncessary leading chars\n","def tokens_to_text_without_leading_chars(token_list):\n","  output=\"\"\n","  for i, token in enumerate(token_list):\n","    if i == 0:\n","      token_parts = token.split()\n","      if (len(token_parts) > 1):\n","        for token_part in token_parts[1:]:\n","          output += token_part\n","      else:\n","        output += token.strip()\n","    else:\n","      output += token\n","\n","  return output\n","\n","# extract the claims from the tokens and the indices\n","def tokens_to_claim_text(token_list, span_start_index, span_end_index):\n","  output = []\n","\n","  for claim_span in zip(span_start_index, span_end_index):\n","    if (claim_span[0] < claim_span[1]):\n","      output.append(tokens_to_text_without_leading_chars(token_list[claim_span[0] : claim_span[1]+1]))\n","\n","  return output\n","\n","# take a dataframe and convert the column with tokens to a column with tweet strings --> returns the new dataframe\n","def new_df_with_strings(df):\n","  tweets = []\n","  for token_list in df[\"tokens\"].values:\n","    tweet = tokens_to_text(token_list)\n","    tweets.append(tweet)\n","\n","  df[\"tweets\"] = tweets\n","  df = df[[\"tweets\", \"tokens\", \"claim_label\", \"span_start_index\", \"span_end_index\"]]\n","\n","  return df\n","\n","# take a dataframe and add a column with a list of claims made in the tweet\n","def new_df_with_claim_text(df):\n","  claims = []\n","  for i in range(df.shape[0]):\n","    claim = tokens_to_claim_text(df[\"tokens\"][i], df[\"span_start_index\"][i], df[\"span_end_index\"][i])\n","    claims.append(claim)\n","\n","  df[\"claims\"] = claims\n","\n","  return df"],"metadata":{"id":"TgjSfbVSQmv7","executionInfo":{"status":"ok","timestamp":1686703761293,"user_tz":420,"elapsed":4,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["# read original files\n","df_train = pd.read_csv(\"../data/train.csv\", converters={'tokens':literal_eval, 'span_start_index':literal_eval, 'span_end_index':literal_eval})\n","df_dev = pd.read_csv(\"../data/dev.csv\", converters={'tokens':literal_eval, 'span_start_index':literal_eval, 'span_end_index':literal_eval})"],"metadata":{"id":"6zTXhgoOPyDS","executionInfo":{"status":"ok","timestamp":1686703420168,"user_tz":420,"elapsed":840,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["# add tweets columns\n","df_train_with_tweets = new_df_with_strings(df_train)\n","df_dev_with_tweets = new_df_with_strings(df_dev)\n","\n","# add claim lists\n","df_train_with_claims = new_df_with_claim_text(df_train)\n","df_dev_with_claims = new_df_with_claim_text(df_dev)\n","\n","# save new df's\n","df_train_with_claims.to_csv(\"../data/train-T5.csv\", index=False)\n","df_dev_with_claims.to_csv(\"../data/dev-T5.csv\", index=False)"],"metadata":{"id":"ctoBBM2IQGQG","executionInfo":{"status":"ok","timestamp":1686703886381,"user_tz":420,"elapsed":942,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"execution_count":80,"outputs":[]}]}