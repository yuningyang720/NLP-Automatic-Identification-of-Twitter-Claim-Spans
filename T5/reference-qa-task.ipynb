{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"provenance":[{"file_id":"https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb","timestamp":1686453166775}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"LJzDxo7aIpzY"},"source":["# T5 for Sentiment Span Extraction\n","\n","This introduction [notebook](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) is featured in Abhishek Thakur's Talks #3 webinar on [youtube](https://www.youtube.com/watch?v=4LYw_UIdd4A).\n","\n","## Goals for this talk\n","\n","1. Introduce T5 and how it works\n","2. Explain T5's significance for the future of NLP\n","3. Illustrate how to use T5 for Sentiment Span Extraction\n","\n","\n","## T5 Overview\n","\n","T5 is a recently released encoder-decoder model that reaches SOTA results by solving NLP problems with a text-to-text approach. This is where text is used as both an input and an output for solving all types of tasks. This was introduced in the recent paper, *Exploring the Limits of Transfer Learning with a\n","Unified Text-to-Text Transformer* ([paper](https://arxiv.org/pdf/1910.10683.pdf)). I've been deeply interested in this model the moment I read about it.\n","\n","I believe that the combination of text-to-text as a universal interface for NLP tasks paired with multi-task learning (single model learning multiple tasks) will have a huge impact on how NLP deep learning is applied in practice.\n","\n","In this presentation I aim to give a brief overview of T5, explain some of its implications for NLP in industry, and demonstrate how it can be used for sentiment span extraction on tweets. I hope this material helps you guys use T5 for your own purposes!\n","\n","\n","## Key points from T5 paper\n","\n","<img src=\"https://drive.google.com/uc?id=15XQ-H7IdT3DVtbL7fgwYIm08OYWbB8VZ\" width=\"700\" height=\"300\" />\n","\n","1. **Treats each NLP problem as a “text-to-text” problem and reaches SOTA results** - input: text, output: text\n","\n","2. **Unified approach for NLP Deep Learning** - Since the task is reflected purely in the text input and output, you can use the same model, objective, training procedure, and decoding process to ANY task. Above framework can be used for any task - show Q&A, summarization, etc.\n","\n","3. **Multiple NLP tasks can live in the same model** - E.g. Q&A, semantic similarity, etc. However, there is a problem called *task interference* where good results on one task can also mean worse results on another task. E.g., a good summarizer may be bad at Q&A and vice versa. All the tasks above can live in the same model, which is how it works with the released T5 models (t5-small, t5-base, etc.)\n","\n","#### Multi-task pre-training + Fine-tuning accuracy for different T5 sizes\n","\n","<img src=\"https://drive.google.com/uc?id=1-1SXVF78l3EbsuTeDUEv0hUvgfMIch3f\" width=\"700\" height=\"165\" />\n","\n","4. **New dataset: “Colossal Clean Crawled Corpus” (C4)** - a dataset consisting of ~750GB of clean English text scraped from the web. Created with a month of data from the Common Crawl corpus cleaned with a set of heuristics to filter out \"unhelpful\" text (e.g. offensive language, placeholder text, source code). This is a lot larger than the 13GB of data used for BERT, and 126GB of data used for XLNet.\n","\n","\n","5. **A simple denoising training objective was used for pretraining** Basically, masked language modelling but while considering contiguous masks as a single “span” to predict, and where the final prediction is an actual text sequence containing the answers (represented by “sentinel tokens”). This was compared to a language modeling pre-training objective and results consistently improved.\n","\n","<img src=\"https://drive.google.com/uc?id=1iPG7UxZPvy6c2iwixQXYetpTzcHiKDuW\" width=\"500\" height=\"200\" />\n","\n","6. **Full encoder-decoder transformer architecture is used** - this is in contrast to previous architectures that were either encoder based (e.g. BERT), or decoder based (e.g. GPT-2). This was found effective for both generation & classification tasks.\n","\n","<img src=\"https://drive.google.com/uc?id=1tUxL_os-pn8JTBSCY6l-9rQI0EMPC5Zz\" width=\"400\" height=\"500\" />\n","\n","\n","## Key insight\n","\n","Multiple NLP tasks can be learned by a single model since every NLP problem can be represented in a unified way - as a controllable text generation problem.\n","\n","## Expected impact\n","\n","Increased adoption of multi-task models like T5 due to SOTA accuracy paired with lower time, compute, & storage costs for both deployments and experiments in NLP.\n","\n","## T5 for Sentiment Span Extraction (PyTorch)\n","\n","1. This is a dataset from an existing Kaggle [competition](https://www.kaggle.com/c/tweet-sentiment-extraction/data) - Tweet Sentiment Extraction\n","2. Most of the existing model implementations use some sort of token classification task\n","  - The index of the beginning and ending tokens are predicted and use to *extract* the span\n","\n","3. T5 is an approach that is purely *generative*, like a classic language modelling task\n","  - This is similar to abstractize summarization, translation, and overall text generation\n","  - For our data, the span is not *extracted* by predicting indices, but by generating the span from scratch\n","\n","## Let's get started!\n","\n","-----------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"aNNTPDxGdbwV"},"source":["Related links\n","1. [T5 Paper](https://arxiv.org/pdf/1910.10683.pdf) - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n","2. [T5 Implementation on PyTorch](https://github.com/huggingface/transformers/blob/455c6390938a5c737fa63e78396cedae41e4e87e/src/transformers/modeling_t5.py) by HuggingFace\n","3. [T5 Exploration Notebook](https://colab.research.google.com/drive/1dYqSqq4OCDV0nN3gkagjqXoWX8ZqrE-7?usp=sharing) demonstrating how to use t5-base for summarization and Q&A\n","4. [SQuAD Dataset](https://rajpurkar.github.io/SQuAD-explorer/) for extractive Q&A\n","5. [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction) (Kaggle Competition)\n","6. [Bert Base Uncased for Sentiment Span Extraction (Token Classification)](https://www.kaggle.com/enzoamp/commented-bert-base-uncased-using-pytorch/comments) (Commented version of Abhishek Thakur's solution)"]},{"cell_type":"markdown","metadata":{"id":"ghUVyHh8Hhf0"},"source":["## Installation\n","\n","Installing the [transformers](https://github.com/huggingface/transformers) + [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) libraries. The rest of the required packages are already installed in Google Colab by default.\n","\n","I'd describe PyTorch Lightning as a PyTorch wrapper that simplifies the process of writing PyTorch code."]},{"cell_type":"code","metadata":{"id":"k9BjtREAMd5A","outputId":"ce1fa0e8-8c28-4a77-f0c1-e9431224227e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686454286034,"user_tz":420,"elapsed":154400,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["!pip install transformers\n","!pip install git+git://github.com/williamFalcon/pytorch-lightning.git@master --upgrade"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+git://github.com/williamFalcon/pytorch-lightning.git@master\n","  Cloning git://github.com/williamFalcon/pytorch-lightning.git (to revision master) to /tmp/pip-req-build-1s31a10z\n","  Running command git clone --filter=blob:none --quiet git://github.com/williamFalcon/pytorch-lightning.git /tmp/pip-req-build-1s31a10z\n","  fatal: unable to connect to github.com:\n","  github.com[0: 140.82.113.3]: errno=Connection timed out\n","\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet git:\u001b[0m\u001b[32m/\u001b[0m\u001b[32m/github.com/williamFalcon/\u001b[0m\u001b[32mpytorch-lightning.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-1s31a10z\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet git:\u001b[0m\u001b[32m/\u001b[0m\u001b[32m/github.com/williamFalcon/\u001b[0m\u001b[32mpytorch-lightning.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-1s31a10z\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"markdown","metadata":{"id":"8B-AcCZVqMBL"},"source":["## Downloading data with CLI\n","\n","The dataset I'm using actually comes from an existing Kaggle competition called [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)."]},{"cell_type":"code","metadata":{"id":"y1vih3VDqP_p","outputId":"ac0729ad-23b6-4b0a-9687-bb725707dbed","colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"status":"error","timestamp":1686453706626,"user_tz":420,"elapsed":54798,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Run this cell and select the kaggle.json file downloaded\n","# from the Kaggle account settings page.\n","from google.colab import files\n","files.upload()\n","# Makes sure creds aren't printed\n","print()"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-4af170bc-2b98-4cf4-ae72-431178b40094\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-4af170bc-2b98-4cf4-ae72-431178b40094\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-1ab810f4c56d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# from the Kaggle account settings page.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Makes sure creds aren't printed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    154\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    155\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"BbCV5bBLqWZ_","executionInfo":{"status":"aborted","timestamp":1686453706627,"user_tz":420,"elapsed":25,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Let's make sure the kaggle.json file is present.\n","!ls -lha kaggle.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w-Fx2f2LqZDS","executionInfo":{"status":"aborted","timestamp":1686453706628,"user_tz":420,"elapsed":25,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# The Kaggle API client expects this file to be in ~/.kaggle,\n","# so move it there.\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","\n","# This permissions change avoids a warning on Kaggle tool startup.\n","!chmod 600 ~/.kaggle/kaggle.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYRsT-eHqac3","executionInfo":{"status":"aborted","timestamp":1686453706628,"user_tz":420,"elapsed":24,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# List available datasets.\n","!kaggle datasets list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rtBGzY0qboJ","executionInfo":{"status":"aborted","timestamp":1686453706628,"user_tz":420,"elapsed":24,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["!kaggle competitions download -c tweet-sentiment-extraction -p /content/tweet-sentiment-extraction"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rs2BclAuqd1Q","executionInfo":{"status":"aborted","timestamp":1686453706629,"user_tz":420,"elapsed":25,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["%cd /content/tweet-sentiment-extraction\n","!unzip \\*.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kf50gKC-Len0","executionInfo":{"status":"aborted","timestamp":1686453706629,"user_tz":420,"elapsed":24,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["ls /content/tweet-sentiment-extraction"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7P5C7xGaMcUM"},"source":["# Setup\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WFegh1JsvDjQ"},"source":["Reading the data"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"aSKRffoaJYKb","executionInfo":{"status":"aborted","timestamp":1686453706629,"user_tz":420,"elapsed":24,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"auRxG9amJYKw","executionInfo":{"status":"aborted","timestamp":1686453706630,"user_tz":420,"elapsed":25,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Row 314 of train set is nan\n","train = pd.read_csv('/content/tweet-sentiment-extraction/train.csv').dropna()#.head(1000)\n","test = pd.read_csv('/content/tweet-sentiment-extraction/test.csv')#.head(1000)\n","\n","# Set random 13% as the validation set (make validation set similar in size to test set)\n","train, val = train_test_split(train, test_size=0.13, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAyvcozH5kbN","executionInfo":{"status":"aborted","timestamp":1686453706630,"user_tz":420,"elapsed":25,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["train.shape, test.shape, val.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8V06zveCAxTW"},"source":["Confirm that the train set doesn't have any overlaps with the test and validation sets"]},{"cell_type":"code","metadata":{"id":"KcWsXdQ_AcBP","executionInfo":{"status":"aborted","timestamp":1686453706631,"user_tz":420,"elapsed":25,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["set(test.textID.values).intersection(train.textID.values), set(val.textID.values).intersection(train.textID.values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Sb1CYxJPJYKz","executionInfo":{"status":"aborted","timestamp":1686453706632,"user_tz":420,"elapsed":26,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["train.head(), test.head(), val.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s24C6B08j4Fz","executionInfo":{"status":"aborted","timestamp":1686453706633,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["train.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNadgQMdvPOq"},"source":["Checking out how the data looks\n","\n","My biggest concern is that sometimes the span is a single word like \"afraid\", but then sometimes it's a whole phrase, like \"jip i have a good one\". I suspect that this variation may be harder to model with deep learning."]},{"cell_type":"code","metadata":{"id":"Tu5sPDosjldF","executionInfo":{"status":"aborted","timestamp":1686453706633,"user_tz":420,"elapsed":26,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Input\n","for a,b,_ in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n","    print(\"sentiment:\", a, \"tweet:\", b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eym7h9yTkG2-","executionInfo":{"status":"aborted","timestamp":1686453706634,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Target (what we're trying to predict)\n","for _,_,c in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n","    print(c)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"seQYpn4dvW1q"},"source":["Let's see the GPU we get from Colab.\n","\n","If you're lucky, you get a P100, but you will typically get a K80."]},{"cell_type":"code","metadata":{"id":"6krpC4qiMcJ5","executionInfo":{"status":"aborted","timestamp":1686453706634,"user_tz":420,"elapsed":26,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Checking out the GPU we have access to\n","!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kzDzPYzEJYK4"},"source":["## Format data to input text Q&A format\n","\n","For the T5 model, I decided to build on top of the existing Q&A task already learned by T5 since the nature of Q&A based on the SQuAD dataset is **extractive** in nature.\n","\n","In other words, utilizing the Q&A task's text formatting should theoretically allow us to utilize T5's existing knowledge of extracting spans from an input text, which is exactly what we want to do for this sentiment span extraction task."]},{"cell_type":"code","metadata":{"trusted":true,"id":"bka57iFhJYK5","executionInfo":{"status":"aborted","timestamp":1686453706634,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Checking for NaNs\n","train.isna().sum().sum(), test.isna().sum().sum(), val.isna().sum().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"4VQaCWw5JYK-","executionInfo":{"status":"aborted","timestamp":1686453706635,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Append EOS token to target text\n","# This is the standard format for T5 targets\n","# More info in transformers docs: https://huggingface.co/transformers/model_doc/t5.html\n","train['selected_text'] = train['selected_text'] + ' </s>'\n","val['selected_text'] = val['selected_text'] + ' </s>'\n","\n","# Apply Q&A structure\n","# From Appendix D in the T5 paper\n","processed_input_train = (\"question: \" + train.sentiment + \" context: \" + train.text)\n","processed_input_test = (\"question: \" + test.sentiment + \" context: \" + test.text)\n","processed_input_val = (\"question: \" + val.sentiment + \" context: \" + val.text)\n","\n","# Save data as string separated by \\n (new line)\n","processed_input_str_train = '\\n'.join(processed_input_train.values.tolist())\n","processed_input_str_test = '\\n'.join(processed_input_test.values.tolist())\n","selected_text_str_train = '\\n'.join(train['selected_text'].values.tolist())\n","processed_input_str_val = '\\n'.join(processed_input_val.values.tolist())\n","selected_text_str_val = '\\n'.join(val['selected_text'].values.tolist())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"IwcTHBxsJYLF","executionInfo":{"status":"aborted","timestamp":1686453706636,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["processed_input_train[0], train['selected_text'][0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"QxufgYo7JYLJ","executionInfo":{"status":"aborted","timestamp":1686453706636,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["processed_input_test[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLtuUUYFJYLM"},"source":["## Save input files\n","\n","We save the inputs as text files since our DataLoader class takes in data in this format."]},{"cell_type":"code","metadata":{"trusted":true,"id":"l-8oZk-pJYLN","executionInfo":{"status":"aborted","timestamp":1686453706637,"user_tz":420,"elapsed":28,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Save source files\n","\n","with open('/content/train.source', 'w') as f:\n","    f.write(processed_input_str_train)\n","    \n","# Making dev similar in this case\n","with open('/content/test.source', 'w') as f:\n","    f.write(processed_input_str_test)\n","    \n","with open('/content/val.source', 'w') as f:\n","    f.write(processed_input_str_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"aRpwgJE8JYLQ","executionInfo":{"status":"aborted","timestamp":1686453706637,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["!head /content/train.source"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"AKG0fleVJYLV","executionInfo":{"status":"aborted","timestamp":1686453706637,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["!head /content/test.source"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"xEdikCY9JYLY","executionInfo":{"status":"aborted","timestamp":1686453706638,"user_tz":420,"elapsed":28,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["!head /content/val.source"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8pQJfXOkJYLb"},"source":["# Save target file"]},{"cell_type":"code","metadata":{"trusted":true,"id":"Ip_sb89HJYLb","executionInfo":{"status":"aborted","timestamp":1686453706638,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["with open('/content/train.target', 'w') as f:\n","    f.write(selected_text_str_train)\n","    \n","with open('/content/val.target', 'w') as f:\n","    f.write(selected_text_str_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"fjx-bSx2JYLg","executionInfo":{"status":"aborted","timestamp":1686453706638,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["!head /content/train.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"WU94HdcVJYLl","executionInfo":{"status":"aborted","timestamp":1686453706638,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["!head /content/val.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"CwmOitjOJYLo","executionInfo":{"status":"aborted","timestamp":1686453706639,"user_tz":420,"elapsed":28,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["ls /content/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rZORta1ZJYLq"},"source":["## Preparing the T5 Dataset\n","\n","Based on this summarization example from [transformers](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/finetune.py), which is compatible for both BART and T5"]},{"cell_type":"code","metadata":{"trusted":true,"id":"bzJjYWOtJYLq","executionInfo":{"status":"aborted","timestamp":1686453706639,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["import os\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","from transformers.tokenization_utils import trim_batch\n","\n","\n","def encode_file(tokenizer, data_path, max_length, pad_to_max_length=True, return_tensors=\"pt\"):\n","    \"\"\"\n","    This function reads the text files that we prepared and returns them in tokenized form.\n","\n","    Actually tokenizer.batch_encode_plus returns these as a list of dictionaries where \n","    each dictionary contains the word piece indices among other relevant inputs for training & inference\n","    \"\"\"\n","    examples = []\n","    with open(data_path, \"r\") as f:\n","        for text in f.readlines():\n","            tokenized = tokenizer.batch_encode_plus(\n","                [text], max_length=max_length, pad_to_max_length=pad_to_max_length, return_tensors=return_tensors,\n","            )\n","            examples.append(tokenized)\n","    return examples\n","\n","\n","class T5Dataset(Dataset):\n","    \"\"\"\n","    This is the T5 dataset that can read our train, test, and dev files separately\n","\n","    This was patterned after the SummarizationDataset from the `transformer` library's summarization example (compatible with T5)\n","    \"\"\"\n","    def __init__(\n","        self,\n","        tokenizer,\n","        data_dir=\"../working/\",\n","        type_path=\"train\",\n","        max_source_length=1024,\n","        max_target_length=56,\n","    ):\n","        super().__init__()\n","        # Store the tokenizer\n","        self.tokenizer = tokenizer\n","        self.type_path = type_path\n","        # Read the source and target files for the type of file (train, test, or val)\n","        self.source = encode_file(tokenizer, os.path.join(data_dir, type_path + \".source\"), max_source_length)\n","        self.target = None\n","        if self.type_path != \"test\":\n","            self.target = encode_file(tokenizer, os.path.join(data_dir, type_path + \".target\"), max_target_length)\n","\n","    def __len__(self):\n","        return len(self.source)\n","\n","    def __getitem__(self, index):\n","        # Return example as a dictionary containing source_ids, src_mask, and target_ids\n","        source_ids = self.source[index][\"input_ids\"].squeeze() # (1024,)\n","        # We need masks for transformers to:\n","        # 1) ignore padding for both the encoder and decoder stages (src_mask)\n","        # 2) ignore future tokens at the decoder stage\n","        src_mask = self.source[index][\"attention_mask\"].squeeze()\n","\n","        if self.type_path == \"test\":\n","            return {\"source_ids\": source_ids, \"source_mask\": src_mask}\n","\n","        target_ids = self.target[index][\"input_ids\"].squeeze() # (56, )\n","        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids}\n","\n","    # Static methods, much like class methods, are methods that are bound to a class rather than its object.\n","    # They do not require a class instance creation. So, they are not dependent on the state of the object.\n","    # https://www.programiz.com/python-programming/methods/built-in/staticmethod\n","    @staticmethod\n","    def trim_seq2seq_batch(batch, pad_token_id, test=False):\n","        # Remove columns that are populated exclusively by pad_token_id\n","        # This ensures that each batch is padded only uptil the \"max sequence length\"\n","        # https://github.com/huggingface/transformers/blob/1e51bb717c04ca4b01a05a7a548e6b550be38628/src/transformers/tokenization_utils.py\n","        source_ids, source_mask = trim_batch(batch[\"source_ids\"], pad_token_id, attention_mask=batch[\"source_mask\"])\n","        if test:\n","            return source_ids, source_mask, None\n","        y = trim_batch(batch[\"target_ids\"], pad_token_id)\n","        return source_ids, source_mask, y\n","\n","    def collate_fn(self, batch):\n","        \"\"\"\n","        The tensors are stacked together as they are yielded.\n","\n","        Collate function is applied to the output of a DataLoader as it is yielded.\n","        \"\"\"\n","        input_ids = torch.stack([x[\"source_ids\"] for x in batch]) # BS x SL\n","        masks = torch.stack([x[\"source_mask\"] for x in batch]) # BS x SL\n","        pad_token_id = self.tokenizer.pad_token_id\n","        source_ids, source_mask = trim_batch(input_ids, pad_token_id, attention_mask=masks)\n","        if self.type_path == \"test\":\n","            return {\"source_ids\": source_ids, \"source_mask\": source_mask}\n","\n","        target_ids = torch.stack([x[\"target_ids\"] for x in batch]) # BS x SL\n","        # Remove columns that are purely padding\n","        y = trim_batch(target_ids, pad_token_id)\n","        # Return dictionary containing tensors\n","        return {\"source_ids\": source_ids, \"source_mask\": source_mask, \"target_ids\": y}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"im8yu0a3JYLt"},"source":["## Model\n","\n","Based on this summarization (encoder-decoder) example from [transformers](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/finetune.py), which is compatible for both BART and T5"]},{"cell_type":"code","metadata":{"trusted":true,"id":"HM4stMDvJYLt","executionInfo":{"status":"aborted","timestamp":1686453706639,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["import argparse\n","import logging\n","import os\n","import random\n","\n","import numpy as np\n","import pytorch_lightning as pl\n","import torch\n","from torch.utils.data import DataLoader\n","\n","from transformers import (\n","    AdamW,\n","    AutoConfig,\n","    AutoModelWithLMHead,\n","    AutoTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","def set_seed(args: argparse.Namespace):\n","    \"\"\"\n","    Set all the seeds to make results replicable\n","    \"\"\"\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","def jaccard(str1, str2): \n","    a = set(str1.lower().split()) \n","    b = set(str2.lower().split())\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","\n","class T5Module(pl.LightningModule):\n","    \"\"\"\n","    Base Transformer model that uses Pytorch Lightning as a PyTorch wrapper.\n","\n","    T5 specific methods are implemented in T5Trainer\n","    \"\"\"\n","    def __init__(self, hparams: argparse.Namespace, **config_kwargs):\n","        \"Initialize a model.\"\n","\n","        super().__init__()\n","        self.hparams = hparams\n","        cache_dir = self.hparams.cache_dir if self.hparams.cache_dir else None\n","        # Read the config file of the T5 model (T5Config)\n","        # AutoConfig allows you to read the configuration for a specified model (e.g. in this case, t5-base)\n","        # Reference: https://huggingface.co/transformers/model_doc/auto.html#autoconfig\n","        self.config = AutoConfig.from_pretrained(self.hparams.model_name_or_path)\n","        # Read the tokenizer of the T5 model (T5Tokenizer)\n","        # AutoTokenizer allows you to read the tokenizer for a specified model (e.g. in this case, t5-base)\n","        # Reference: https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(\n","            self.hparams.model_name_or_path,\n","            cache_dir=cache_dir,\n","        )\n","        # Read the model file for the pre-trained T5 model (T5ForConditionalGeneration)\n","        # AutoModelWithLMHead allows you to read any of the language modelling models from the transformers library (e.g. in this case, t5-base)\n","        # Automodels reference: https://huggingface.co/transformers/model_doc/auto.html#automodel\n","        self.model = AutoModelWithLMHead.from_pretrained(\n","            self.hparams.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in self.hparams.model_name_or_path), # Checkpoint is a TF format\n","            config=self.config,\n","            cache_dir=cache_dir,\n","        )\n","\n","        # Save dataset params\n","        self.dataset_kwargs: dict = dict(\n","            data_dir=self.hparams.data_dir,\n","            max_source_length=self.hparams.max_source_length,\n","            max_target_length=self.hparams.max_target_length,\n","        )\n","\n","    # Forward function\n","    # Defines the forward pass of the module\n","\n","    def forward(\n","        self,\n","        input_ids, # Indices of input sequence tokens in the vocabulary. \n","        attention_mask=None, # Mask to avoid performing attention on padding token indices\n","        decoder_input_ids=None, # T5 uses the pad_token_id as the starting token for decoder_input_ids generation.\n","        lm_labels=None # Labels for computing the sequence classification/regression loss (see T5Model). Note: loss is returned when lm_label is provided.\n","        ):\n","        \"\"\"\n","         loss (torch.FloatTensor of shape (1,), optional, returned when lm_label is provided\n","        \"\"\"\n","        # Details on how to use this in the Hugging Face T5 docs: https://huggingface.co/transformers/model_doc/t5.html\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            lm_labels=lm_labels,\n","        )\n","\n","    # Data preparation\n","\n","    def get_dataloader(self, type_path: str, batch_size: int, shuffle: bool = False) -> DataLoader:\n","        dataset = T5Dataset(self.tokenizer, type_path=type_path, **self.dataset_kwargs)\n","        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=shuffle)\n","        return dataloader\n","\n","    def train_dataloader(self) -> DataLoader:\n","        dataloader = self.get_dataloader(\"train\", batch_size=self.hparams.train_batch_size, shuffle=True)\n","        t_total = (\n","            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n","            // self.hparams.gradient_accumulation_steps\n","            * float(self.hparams.num_train_epochs)\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n","        )\n","        self.lr_scheduler = scheduler\n","        return dataloader\n","\n","    def val_dataloader(self) -> DataLoader:\n","        return self.get_dataloader(\"val\", batch_size=self.hparams.eval_batch_size)\n","\n","    def test_dataloader(self) -> DataLoader:\n","        return self.get_dataloader(\"test\", batch_size=self.hparams.eval_batch_size)\n","\n","    # Configure optimizers\n","\n","    def configure_optimizers(self):\n","        \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","        model = self.model\n","        # Weight decay explanation:\n","        # Weight decay will not be applied to \"bias\" and \"LayerNorm.weight\" parameters\n","        # When training neural networks, it is common to use \"weight decay,\" where after each update,\n","        # the weights are multiplied by a factor slightly less than 1.\n","        # This prevents the weights from growing too large, and can be seen as gradient descent on a quadratic regularization term.\n","        # https://metacademy.org/graphs/concepts/weight_decay_neural_networks\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","\n","        # Group parameters to those that will and will not have weight decay applied\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparams.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        # Use AdamW as an optimizer\n","        # Intro here: https://www.fast.ai/2018/07/02/adam-weight-decay/\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","        self.opt = optimizer\n","        return [optimizer]\n","\n","    # Forward pass and calculate loss per batch (step)\n","\n","    def _step(self, batch, return_text=False):\n","        \"\"\"\n","        Runs forward pass and calculates loss per batch. Applied for training_step, and validation_step\n","        \"\"\"\n","        pad_token_id = self.tokenizer.pad_token_id\n","        source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone()\n","        # Change pad_token_id to -100\n","        lm_labels[y[:, 1:] == pad_token_id] = -100\n","        # Run forward pass and calculate loss\n","        outputs = self(source_ids, attention_mask=source_mask, decoder_input_ids=y_ids, lm_labels=lm_labels,)\n","        # Only get loss from the output since that's all we need to apply our optimizer\n","        loss = outputs[0]\n","        if return_text:\n","            target_text = [self.tokenizer.decode(ids) for ids in y_ids]\n","            return loss, target_text\n","        else:\n","            return loss\n","\n","    # Step during training\n","\n","    def training_step(self, batch, batch_idx):\n","        \"\"\"\n","        Runs forward pass, calculates loss, and returns loss (and logs) in a dict\n","        \"\"\"\n","        loss = self._step(batch)\n","\n","        # Notice that each training step loss is recorded on tensorboard, which makes sense since we're tracking loss per batch\n","        tensorboard_logs = {\"train_loss\": loss}\n","        return {\"loss\": loss, \"log\": tensorboard_logs}\n","\n","    # Adjust weights based on calculated gradients and learning rate scheduler\n","\n","    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n","        \"\"\"\n","        Adjust weights based on calculated gradients + learning rate scheduler, and refresh gradients\n","        Reference for optimizer_step: https://pytorch-lightning.readthedocs.io/en/latest/optimizers.html\n","        \"\"\"\n","        if self.trainer.use_tpu:\n","            xm.optimizer_step(optimizer)\n","        else:\n","            # Adjust weights based on calculated gradients\n","            optimizer.step()\n","\n","        # Refresh gradients (to zero)\n","        optimizer.zero_grad()\n","        # Update the learning rate scheduler\n","        self.lr_scheduler.step()\n","\n","    # Step during validation\n","\n","    def validation_step(self, batch, batch_idx):\n","        \"\"\"\n","        Runs forward pass, calculates loss, and returns loss in a dict\n","        \"\"\"\n","\n","        # Return source and target text to calculate jaccard score only for validation\n","        loss, target_text = self._step(batch, return_text=True)\n","\n","        preds = self.test_step(batch, batch_idx)\n","        preds_text = preds[\"preds\"]\n","        # Track jaccard score to get validation accuracy\n","        jaccard_score = [jaccard(p, t) for p, t in zip(preds_text, target_text)]\n","\n","        return {\"val_loss\": loss, \"jaccard_score\": jaccard_score}\n","\n","    # Show loss after validation\n","\n","    def validation_end(self, outputs):\n","        \"\"\"\n","        Calculate average loss for all the validation batches\n","        \"\"\"\n","        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","        jaccard_scores = sum([x[\"jaccard_score\"] for x in outputs], [])\n","        avg_jaccard_score = np.mean(jaccard_scores)\n","        tensorboard_logs = {\"val_loss\": avg_loss, \"jaccard_score\": avg_jaccard_score}\n","        return {\"avg_val_loss\": avg_loss, \"avg_jaccard_score\": avg_jaccard_score, \"log\": tensorboard_logs}\n","\n","    # Step during testing\n","\n","    def test_step(self, batch, batch_idx):\n","        \"\"\"\n","        Runs forward pass on test set and returns calculated loss, predictions, and targets\n","        Note: this assumes that your test set has targets (doesn't have for kaggle).\n","        \"\"\"\n","        pad_token_id = self.tokenizer.pad_token_id\n","        source_ids, source_mask, _ = T5Dataset.trim_seq2seq_batch(batch, pad_token_id, test=True)\n","        # NOTE: the following kwargs get more speed and lower quality summaries than those in evaluate_cnn.py\n","        # Generate reference: https://github.com/huggingface/transformers/blob/3e0f06210646a440509efa718b30d18322d6a830/src/transformers/modeling_utils.py#L769\n","        # For the sentiment span extraction task, turning off early stopping proved superior\n","        generated_ids = self.model.generate(\n","            input_ids=source_ids,\n","            attention_mask=source_mask,\n","            num_beams=1,\n","            max_length=80,\n","            repetition_penalty=2.5,\n","            length_penalty=1.0,\n","            early_stopping=True,\n","            use_cache=True,\n","        )\n","        preds = [\n","            self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","            for g in generated_ids\n","        ]\n","\n","        return {\"preds\": preds}\n","\n","    # Note: we don't attempt to print the loss from the test set, because it's assumed that we don't have the test targets\n","    def test_end(self, outputs):\n","        \"\"\"\n","        \"\"\"\n","        preds = []\n","        for pred in outputs:\n","            preds += pred[\"preds\"]\n","        return {\"preds\": preds}\n","\n","    def test_epoch_end(self, outputs):\n","        \"\"\"\n","        Save test predictions and targets as text files and return the calculated loss for the test set\n","        \"\"\"\n","        output_test_predictions_file = os.path.join(self.hparams.output_dir, \"test_predictions.txt\")\n","        # write predictions and targets for later rouge evaluation.\n","        with open(output_test_predictions_file, \"w+\") as p_writer:\n","            for output_batch in outputs:\n","                p_writer.writelines(s + \"\\n\" for s in output_batch[\"preds\"])\n","            p_writer.close()\n","\n","        return self.test_end(outputs)\n","\n","    def get_tqdm_dict(self):\n","        \"\"\"\n","        Print average loss and learning rate at each step\n","        \"\"\"\n","        avg_loss = getattr(self.trainer, \"avg_loss\", 0.0)\n","        tqdm_dict = {\"loss\": \"{:.3f}\".format(avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","        return tqdm_dict\n","\n","    def _feature_file(self, mode):\n","        return os.path.join(\n","            self.hparams.data_dir,\n","            \"cached_{}_{}_{}\".format(\n","                mode,\n","                list(filter(None, self.hparams.model_name_or_path.split(\"/\"))).pop(),\n","                str(self.hparams.max_seq_length),\n","            ),\n","        )\n","\n","    def is_logger(self):\n","        return self.trainer.proc_rank <= 0\n","\n","    @staticmethod\n","    def add_model_specific_args(parser, root_dir):\n","        parser.add_argument(\n","            \"--model_name_or_path\",\n","            default=None,\n","            type=str,\n","            required=True,\n","            help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n","        )\n","        parser.add_argument(\n","            \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n","        )\n","        parser.add_argument(\n","            \"--tokenizer_name\",\n","            default=\"\",\n","            type=str,\n","            help=\"Pretrained tokenizer name or path if not the same as model_name\",\n","        )\n","        parser.add_argument(\n","            \"--cache_dir\",\n","            default=\"\",\n","            type=str,\n","            help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","        )\n","        parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n","        parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n","        parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","        parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n","        parser.add_argument(\n","            \"--num_train_epochs\", default=3, type=int, help=\"Total number of training epochs to perform.\"\n","        )\n","\n","        parser.add_argument(\"--train_batch_size\", default=32, type=int)\n","        parser.add_argument(\"--eval_batch_size\", default=32, type=int)\n","\n","        parser.add_argument(\n","            \"--max_source_length\",\n","            default=1024,\n","            type=int,\n","            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\",\n","        )\n","        parser.add_argument(\n","            \"--max_target_length\",\n","            default=56,\n","            type=int,\n","            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\",\n","        )\n","\n","        parser.add_argument(\n","            \"--data_dir\",\n","            default=None,\n","            type=str,\n","            required=True,\n","            help=\"The input data dir. Should contain the dataset files for the text generation task.\",\n","        )\n","        return parser\n","\n","\n","class LoggingCallback(pl.Callback):\n","    def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n","        logger.info(\"***** Validation results *****\")\n","        if pl_module.is_logger():\n","            metrics = trainer.callback_metrics\n","            # Log results\n","            for key in sorted(metrics):\n","                if key not in [\"log\", \"progress_bar\"]:\n","                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","\n","def add_generic_args(parser, root_dir):\n","    parser.add_argument(\n","        \"--output_dir\",\n","        default=None,\n","        type=str,\n","        required=True,\n","        help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--fp16\",\n","        action=\"store_true\",\n","        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    )\n","\n","    parser.add_argument(\n","        \"--fp16_opt_level\",\n","        type=str,\n","        default=\"O1\",\n","        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","        \"See details at https://nvidia.github.io/apex/amp.html\",\n","    )\n","\n","    parser.add_argument(\"--n_gpu\", type=int, default=1)\n","    parser.add_argument(\"--n_tpu_cores\", type=int, default=0)\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\",\n","        type=int,\n","        default=1,\n","        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","\n","def generic_train(model: T5Module, args: argparse.Namespace):\n","    # init model\n","    set_seed(args)\n","\n","    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n","        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n","\n","    # Can take out checkpoint saving after each epoch to save memory\n","    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","        filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n","    )\n","\n","    train_params = dict(\n","        accumulate_grad_batches=args.gradient_accumulation_steps,\n","        gpus=args.n_gpu,\n","        max_epochs=args.num_train_epochs,\n","        early_stop_callback=False,\n","        gradient_clip_val=args.max_grad_norm,\n","        checkpoint_callback=checkpoint_callback,\n","        callbacks=[LoggingCallback()],\n","    )\n","\n","    if args.fp16:\n","        train_params[\"use_amp\"] = args.fp16\n","        train_params[\"amp_level\"] = args.fp16_opt_level\n","\n","    if args.n_tpu_cores > 0:\n","        global xm\n","        import torch_xla.core.xla_model as xm\n","\n","        train_params[\"num_tpu_cores\"] = args.n_tpu_cores\n","        train_params[\"gpus\"] = 0\n","\n","    if args.n_gpu > 1:\n","        train_params[\"distributed_backend\"] = \"ddp\"\n","\n","    trainer = pl.Trainer(**train_params)\n","\n","    if args.do_train:\n","        trainer.fit(model)\n","\n","    return trainer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5wgSA-VJYLw"},"source":["## Finetuning\n","\n","Based on this summarization example from [transformers](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/finetune.py), which is compatible for both BART and T5"]},{"cell_type":"code","metadata":{"trusted":true,"id":"Q7lrXolxJYLx","executionInfo":{"status":"aborted","timestamp":1686453706640,"user_tz":420,"elapsed":28,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["import argparse\n","import glob\n","import logging\n","import os\n","import time\n","\n","logging.basicConfig(level = logging.INFO)\n","\n","logger = logging.getLogger(__name__)\n","\n","def main(args):\n","\n","    # If output_dir not provided, a folder will be generated in pwd\n","    if not args.output_dir:\n","        args.output_dir = os.path.join(\"./results\", f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",)\n","        os.makedirs(args.output_dir)\n","    model = T5Module(args)\n","    trainer = generic_train(model, args)\n","\n","    # Save the last model as model.bin\n","    #checkpoints = list(sorted(glob.glob(os.path.join(args.output_dir, \"checkpointepoch=*.ckpt\"), recursive=True)))\n","    #model = model.load_from_checkpoint(checkpoints[-1])\n","    model.model.save_pretrained(args.output_dir)\n","    # Save tokenizer files\n","    model.tokenizer.save_pretrained('./')\n","    \n","    # Optionally, predict on dev set and write to output_dir\n","    if args.do_predict:\n","        # See https://github.com/huggingface/transformers/issues/3159\n","        # pl use this format to create a checkpoint:\n","        # https://github.com/PyTorchLightning/pytorch-lightning/blob/master\\\n","        # /pytorch_lightning/callbacks/model_checkpoint.py#L169\n","        trainer.test(model)\n","    return trainer\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Z99mrA4VJYL3","executionInfo":{"status":"aborted","timestamp":1686453706640,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["#!rm -r /content/output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"0_9ICraGJYLz","executionInfo":{"status":"aborted","timestamp":1686453706640,"user_tz":420,"elapsed":27,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["mkdir /content/output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"FO2rxaTvJYL5","executionInfo":{"status":"aborted","timestamp":1686453706640,"user_tz":420,"elapsed":28,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["# Will set gpu on as soon as at least 1 batch works on cpu\n","# TODO: Consider factors here: https://github.com/huggingface/transformers/issues/3387\n","# Change LR to 1e-3 to 1e-4\n","# \n","ARGS_STR = \"\"\"\n","--data_dir=/content/ \\\n","--model_name_or_path=t5-base \\\n","--learning_rate=3e-5 \\\n","--train_batch_size=32 \\\n","--output_dir=/content/output/ \\\n","--do_train \\\n","--n_gpu=1 \\\n","--num_train_epochs 5 \\\n","--max_source_length 80 \\\n","\"\"\"\n","#\n","#--eval_batch_size=3 \\\n","#--do_predict \\\n","\n","parser = argparse.ArgumentParser()\n","add_generic_args(parser, os.getcwd())\n","parser = T5Module.add_model_specific_args(parser, os.getcwd())\n","args = parser.parse_args(ARGS_STR.split())\n","trainer = main(args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37xAGCr9tNkF","executionInfo":{"status":"aborted","timestamp":1686453706641,"user_tz":420,"elapsed":28,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["ls -l /content/output/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mY2yAR1TsF7E","executionInfo":{"status":"aborted","timestamp":1686453706641,"user_tz":420,"elapsed":28,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["ls /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sA1lz-njnSEP","executionInfo":{"status":"aborted","timestamp":1686453706642,"user_tz":420,"elapsed":205614,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["ls /content/lightning_logs/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51Robhj2cVUN","executionInfo":{"status":"aborted","timestamp":1686453706643,"user_tz":420,"elapsed":205602,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["%load_ext tensorboard\n","%tensorboard --logdir lightning_logs/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GPO7iVaLcEwA","executionInfo":{"status":"aborted","timestamp":1686453706643,"user_tz":420,"elapsed":205594,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["#cat lightning_logs/version_0/hparams.yaml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P56Gg3oQTnr1"},"source":["Get model from GCS if already saved"]},{"cell_type":"code","metadata":{"id":"lMqLE-DETuWC","executionInfo":{"status":"aborted","timestamp":1686453706644,"user_tz":420,"elapsed":205593,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["from google.colab import auth"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVkz2rGzTxqx","executionInfo":{"status":"aborted","timestamp":1686453706644,"user_tz":420,"elapsed":205592,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["auth.authenticate_user()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkhwyVSQTx1T","executionInfo":{"status":"aborted","timestamp":1686453706644,"user_tz":420,"elapsed":205584,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["!gsutil cp gs://kaggle-files/tweet-sentiment-extraction/20200507_5epochs/* ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aBK-J-KPJYMI"},"source":["Test if saved model works w sample inference"]},{"cell_type":"code","metadata":{"trusted":true,"id":"lzZnHlDpJYMI","executionInfo":{"status":"aborted","timestamp":1686453706645,"user_tz":420,"elapsed":205584,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"eCHCONzkJYMK","executionInfo":{"status":"aborted","timestamp":1686453706645,"user_tz":420,"elapsed":205583,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","#t5 = T5ForConditionalGeneration.from_pretrained('/content/output/')\n","t5 = T5ForConditionalGeneration.from_pretrained('.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"oT-Fee2kJYMM","executionInfo":{"status":"aborted","timestamp":1686453706646,"user_tz":420,"elapsed":205582,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["def get_span(text):\n","    input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)  # Batch size 1\n","    t5.eval()\n","    generated_ids = t5.generate(\n","        input_ids=input_ids,\n","        num_beams=1,\n","        max_length=80,\n","        #repetition_penalty=2.5\n","    ).squeeze()\n","    predicted_span = tokenizer.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    return predicted_span"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Ny60CpsrJYMP","executionInfo":{"status":"aborted","timestamp":1686453706646,"user_tz":420,"elapsed":205573,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["get_span(\"question: negative context: I`m in VA for the weekend, my youngest son turns 2 tomorrow......it makes me kinda sad, he is getting so big, check out my twipics\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"uvQUGSM2JYMU","executionInfo":{"status":"aborted","timestamp":1686453706646,"user_tz":420,"elapsed":205565,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["get_span(\"question: negative context: Recession hit Veronique Branquinho, she has to quit her company, such a shame!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ngLh3ny0emOv","executionInfo":{"status":"aborted","timestamp":1686453706646,"user_tz":420,"elapsed":205557,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["get_span(\"question: negative context: My bike was put on hold...should have known that.... argh total bummer\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uNdqdgowe0zR","executionInfo":{"status":"aborted","timestamp":1686453706647,"user_tz":420,"elapsed":205551,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["get_span(\"question: positive context: On the monday, so i wont be able to be with you! i love you\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRVGEpS8emVv","executionInfo":{"status":"aborted","timestamp":1686453706647,"user_tz":420,"elapsed":205541,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":["get_span(\"question: positive context: I liked it. Did you record it yourself? If so you have a very soothing voice.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TEQurVqVDYMa"},"source":["## Key results\n","\n","1. Training T5 with 5 epochs gives an accuracy (Jaccard score) of 0.665\n","  - This is still much lower than the 0.714 at the top 10%\n","  - But, much is yet to be done in terms of post training optimization\n","    - Ensembling, stacking, etc\n","2. The amazing thing for me is the confirmation that a generative model like T5 can perform extractive tasks with an accuracy comparable to a token classification version of BERT\n","\n","I'm confident that T5 can reach leaderboard level results with more experiments!"]},{"cell_type":"code","metadata":{"id":"zX-20eSHWgrG","executionInfo":{"status":"aborted","timestamp":1686453706648,"user_tz":420,"elapsed":205541,"user":{"displayName":"Joe Picchi","userId":"15701013625746711775"}}},"source":[],"execution_count":null,"outputs":[]}]}